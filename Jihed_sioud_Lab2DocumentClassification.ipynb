{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abe46be4",
   "metadata": {
    "id": "abe46be4"
   },
   "source": [
    "# Master AI4OneHealth Text Mining Practical Session: Classifying Biomedical Documents with BlueBERT\n",
    "\n",
    "    Aidan Mannion (aidan.mannion@univ-grenoble-alpes.fr)\n",
    "    Didier Schwab (didier.schwab@univ-grenoble-alpes.fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef768f",
   "metadata": {
    "id": "12ef768f"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will walk through the following steps;\n",
    "1. Preparing a dataset of clinical notes for training a language model\n",
    "2. Understanding how pre-trained language models are built and used (transfer learning) and how this procedure can be useful in clinical applications\n",
    "3. Create a text classification model that exploits the hidden representations of an encoder based on the transformer architecture (Ã  la [BERT](https://en.wikipedia.org/wiki/BERT_(language_model)))\n",
    "4. Train this model and test its performance\n",
    "\n",
    "We will be using the following tools & resources for Python;\n",
    "- [PyTorch](https://pytorch.org/docs/stable/index.html) : an open-source library for machine learning\n",
    "- [transformers](https://huggingface.co/docs/transformers/index) by [Hugging Face](https://huggingface.co/): a Pytorch-based library for automatic language processing and in particular Transformer-type neural models (such as BERT)\n",
    "- [tokenizers](https://huggingface.co/docs/tokenizers/python/latest/), also provided by Hugging Face: a Pytorch-based library for tokenisation, explicitly designed to work with the Transformers library\n",
    "- Google Colab, which hosts this Jupyter Notebook. Before starting the work, you may want to consult some introductory pages on [Colab](https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb#scrollTo=YHI3vyhv5p85) or [jupyter notebooks](https://realpython.com/jupyter-notebook-introduction/)\n",
    "\n",
    "\n",
    "#### Further study\n",
    "\n",
    "If you'd like to know more about the Natural Language Processing (NLP) techniques we will be using in this tutorial, feel free to check out the following;\n",
    "- The original [paper](https://arxiv.org/abs/1706.03762) introducing transformer language models, or [this](http://nlp.seas.harvard.edu/2018/04/03/attention.html) great tutorial.\n",
    "- The original [paper](https://arxiv.org/abs/1810.04805v2) introducing BERT, an application of the transformer architecture that broke new ground in NLP.\n",
    "- [BioBERT](https://arxiv.org/abs/1901.08746v4), a version of BERT trained on PubMed articles to adapt it for biomedical tasks.\n",
    "- [ClinicalBERT](https://arxiv.org/abs/1904.03323), a language model that extends BioBERT to clinical applications by training it on text documents from EHR records.\n",
    "- [BLUE](https://arxiv.org/pdf/1906.05474.pdf), a benchmark suite for biomedical language processing tasks (we will be using a BERT model developed as part of this project in this tutorial).\n",
    "\n",
    "\n",
    "#### Installing the Hugging Face libraries\n",
    "\n",
    "Colab runtime instances come with a Python environment with several pre-installed libraries, running on a Linux OS. However, if you are using Colab, you must first install the HuggingFace libraries, which are not pre-installed.\n",
    "\n",
    "To execute a Unix command on a Jupyter Notebook, an exclamation mark must be placed before the command: `!command`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4263088b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4263088b",
    "outputId": "55812243-6cfd-4590-a04e-1e6f5c93ef62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (4.25.1)\n",
      "Requirement already satisfied: tokenizers in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: accelerate in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: requests in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: psutil in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from accelerate) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\ai4oneh\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tokenizers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb55282",
   "metadata": {
    "id": "efb55282"
   },
   "source": [
    "Once these are successfully installed in our Colab environment, we can import the libraries we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab6050e",
   "metadata": {
    "id": "0ab6050e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fe578b",
   "metadata": {
    "id": "b8fe578b"
   },
   "source": [
    "**GPU**\n",
    "\n",
    "Training our neural network requires a lot of matrix calculations. If available, we can use a graphics processor (GPU) to perform these calculations faster. On Colab, you can use a GPU by clicking on _Runtime -> Change runtime type_ and then selecting the \"GPU\" option from the drop-down _Hardware Accelerator_ menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9665cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.], device='cuda:0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros(1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d00000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8701129",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8701129",
    "outputId": "bfbcf1c6-53b5-431f-a9aa-96d6da8b0cdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU available!')\n",
    "    device = torch.cuda.current_device()\n",
    "else:\n",
    "    print('GPU unavailable - CPU will be used for all calculations')\n",
    "    device = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "073ab800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ed14b",
   "metadata": {
    "id": "4d3ed14b"
   },
   "source": [
    "## NLP Application Task: Supervised Document Classification\n",
    "\n",
    "Document classification is a very common application of natural language processing & machine learning techniques. In this paradigm, a \"document\" can refer to any textual data point, from articles to text messages, and \"supervised\" means we are restricting our attention to [supervised ML](https://en.wikipedia.org/wiki/Supervised_learning) - tasks with pre-defined target categories. Some widely-known general uses of document classification in a supervised learning context include spam e-mail detection and recommendation engines.\n",
    "\n",
    "### Medical Document Classification\n",
    "Automatic classification of biomedical and/or clinical documents has the potential to ameliorate many aspects of both practice and research. Potential applications of document classification range from the categorisation of medical articles for literature reviews to the detection of abnormalities in patient discharge reports, and many other tasks that can be time-intensive for medical practitioners.\n",
    "\n",
    "The goal of this tutorial is to use a pre-trained transformer language model provided through the Hugging Face API to train a classifier to associate the relevant hallmarks with PubMed abstracts. This will involve the following steps;\n",
    "- Loading and preprocessing the text corpus\n",
    "- Tokenising the dataset, i.e. using the vocabulary that the language model has learned in the pre-training phase to associate identifiers with the words in our training set\n",
    "- Loading the neural weights of the language model and using them to train a classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e617e18",
   "metadata": {
    "id": "4e617e18"
   },
   "source": [
    "## Dataset: Hallmarks of Cancer\n",
    "\n",
    "The dataset we will be using in this tutorial is the [Hallmarks of Cancer](https://academic.oup.com/bioinformatics/article/32/3/432/1743783) corpus, which contains 1,499 PubMed article abstracts annotated by experts according to their relevance to the ten known _hallmarks of cancer_;\n",
    "- Sustaining proliferative signaling (PS)\n",
    "- Evading growth suppressors (GS)\n",
    "- Resisting cell death (CD)\n",
    "- Enabling replicative immortality (RI)\n",
    "- Inducing angiogenesis (A)\n",
    "- Activating invasion & metastasis (IM)\n",
    "- Genome instability & mutation (GI)\n",
    "- Tumor-promoting inflammation (TPI)\n",
    "- Deregulating cellular energetics (CE)\n",
    "- Avoiding immune destruction (ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eff362f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eff362f2",
    "outputId": "64c129c9-fc86-4efa-c847-774e129b31a5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-12-27 23:50:42--  https://gricad-gitlab.univ-grenoble-alpes.fr/manniona/hoc-dataset/-/archive/main/hoc-dataset-main.zip\n",
      "Resolving gricad-gitlab.univ-grenoble-alpes.fr (gricad-gitlab.univ-grenoble-alpes.fr)... 129.88.175.2\n",
      "Connecting to gricad-gitlab.univ-grenoble-alpes.fr (gricad-gitlab.univ-grenoble-alpes.fr)|129.88.175.2|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 888413 (868K) [application/zip]\n",
      "Saving to: 'hoc.zip'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  5%  431K 2s\n",
      "    50K .......... .......... .......... .......... .......... 11%  995K 1s\n",
      "   100K .......... .......... .......... .......... .......... 17% 6,24M 1s\n",
      "   150K .......... .......... .......... .......... .......... 23%  527K 1s\n",
      "   200K .......... .......... .......... .......... .......... 28% 54,2M 1s\n",
      "   250K .......... .......... .......... .......... .......... 34% 3,16M 1s\n",
      "   300K .......... .......... .......... .......... .......... 40% 1,27M 0s\n",
      "   350K .......... .......... .......... .......... .......... 46%  736K 0s\n",
      "   400K .......... .......... .......... .......... .......... 51% 64,0M 0s\n",
      "   450K .......... .......... .......... .......... .......... 57% 2,77M 0s\n",
      "   500K .......... .......... .......... .......... .......... 63% 21,9M 0s\n",
      "   550K .......... .......... .......... .......... .......... 69% 66,0M 0s\n",
      "   600K .......... .......... .......... .......... .......... 74% 1,05M 0s\n",
      "   650K .......... .......... .......... .......... .......... 80% 3,20M 0s\n",
      "   700K .......... .......... .......... .......... .......... 86%  526K 0s\n",
      "   750K .......... .......... .......... .......... .......... 92% 42,5M 0s\n",
      "   800K .......... .......... .......... .......... .......... 97% 16,8M 0s\n",
      "   850K .......... .......                                    100% 4,28M=0,6s\n",
      "\n",
      "2022-12-27 23:50:43 (1,47 MB/s) - 'hoc.zip' saved [888413/888413]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fetch the dataset\n",
    "!wget --no-check-certificate \"https://gricad-gitlab.univ-grenoble-alpes.fr/manniona/hoc-dataset/-/archive/main/hoc-dataset-main.zip\" -O \"hoc.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d11b26b",
   "metadata": {
    "id": "6d11b26b"
   },
   "outputs": [],
   "source": [
    "# extract the data\n",
    "import zipfile\n",
    "\n",
    "zip_ref = zipfile.ZipFile('hoc.zip', 'r') \n",
    "# Extract all the contents of zip file in current directory\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3d3239",
   "metadata": {
    "id": "5b3d3239"
   },
   "source": [
    "#### Loading the data\n",
    "The corpus is divided into three parts, as is standard practice in machine learning;\n",
    "1. ``train``: (70% of the data) the training set, on which the classifier will learn it's parameters\n",
    "2. ``dev``: (10% of the data) the _development_ set, used to evaluate the generalisation performance of the model as it is trained, and choose better hyperparameter values\n",
    "3. ``test``: (20% of the data) the test or \"hold-out\" set, used only after all training has been completed, to evaluate the \"true\" generalisation performance of the model.\n",
    "\n",
    "For now we will only be working with the training and development sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a19437f3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a19437f3",
    "outputId": "a6e8dfe9-c988-4e2d-e110-73231ac7970f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 10527 rows and 3 columns\n",
      "Development set: 1496 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "train_rawtext = read_csv('hoc-dataset-main/train.tsv', sep='\\t', index_col=0)\n",
    "print(f'Train set: {len(train_rawtext)} rows and {len(train_rawtext.columns)} columns')\n",
    "dev_rawtext = read_csv('hoc-dataset-main/dev.tsv', sep='\\t', index_col=0)\n",
    "print(f'Development set: {len(dev_rawtext)} rows and {len(dev_rawtext.columns)} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e254ae",
   "metadata": {
    "id": "39e254ae"
   },
   "source": [
    "The dataset is in the form of a table with three columns - each row corresponds to a sentence from one of the abstracts, along with the relevant annotation, as follows;\n",
    "1. ``index``: an identifier with the PubMed ID of the abstract and the sentence number of the data point; for example, the third sentence of the abstract with PubMed ID 123456 will have ``1232456_s3`` in this field.\n",
    "2. ``sentence``: the sentence itself.\n",
    "3. ``labels``: either one of the ten abbreviations listed above, corresponding to the hallmark of cancer most relevant to the sentence, or ``NONE``.\n",
    "\n",
    "It is always good to take a look at the first few rows of the dataframe to check that we have all the data that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ad5162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "a1ad5162",
    "outputId": "ebee88f3-c8f1-4f0c-99df-fd22a652b841"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11724768_s0</td>\n",
       "      <td>Ghrelin was identified in the stomach as an en...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11724768_s1</td>\n",
       "      <td>GHS-R is found in various tissues , but its fu...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11724768_s2</td>\n",
       "      <td>Here we show that GHS-R is found in hepatoma c...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11724768_s3</td>\n",
       "      <td>Exposure of these cells to ghrelin caused up-r...</td>\n",
       "      <td>PS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11724768_s4</td>\n",
       "      <td>Unlike insulin , ghrelin inhibited Akt kinase ...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                           sentence labels\n",
       "0  11724768_s0  Ghrelin was identified in the stomach as an en...   NONE\n",
       "1  11724768_s1  GHS-R is found in various tissues , but its fu...   NONE\n",
       "2  11724768_s2  Here we show that GHS-R is found in hepatoma c...   NONE\n",
       "3  11724768_s3  Exposure of these cells to ghrelin caused up-r...     PS\n",
       "4  11724768_s4  Unlike insulin , ghrelin inhibited Akt kinase ...   NONE"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rawtext.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92494772",
   "metadata": {
    "id": "92494772"
   },
   "source": [
    "#### Cleaning the text\n",
    "In order for the tokenizer to properly process the text, we need to remove punctuation and non-alphabetic characters (BERT-style models usually do not deal well with numeric characters). For this we'll use the built-in Python library for dealing with regular expressions (regex), [re](https://docs.python.org/3/library/re.html).\n",
    "\n",
    "The BERT variant we are using in this work was trained on lower-case text, so we also map all capital letters to  lower case. Some of these models do use a vocabulary with capital letters as well; be sure to check the details of the pre-training corpus when experimenting with other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853e0e75",
   "metadata": {
    "id": "853e0e75"
   },
   "outputs": [],
   "source": [
    "# Regex library\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07751a29",
   "metadata": {
    "id": "07751a29"
   },
   "source": [
    "**Exercise:** The ``clean_sentence`` function below is only partially completed. You will need to write the parts that do the following;\n",
    "- remove extra spaces (i.e. ``\" Foo  Bar  \"`` $\\rightarrow$ ``\" Foo Bar \"``)\n",
    "- remove trailing spaces (i.e. ``\" Foo Bar \"`` $\\rightarrow$ ``\"Foo Bar\"``\n",
    "- map to lowercase (i.e. ``\"Foo Bar\"`` $\\rightarrow$ ``\"foo bar\"``)\n",
    "\n",
    "_Hint:_ The ``re.sub`` function used to replace non-alphabetic characters with spaces could be useful again, as well as some of the built-in methods of Python's ``str`` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a64646e9",
   "metadata": {
    "id": "a64646e9"
   },
   "outputs": [],
   "source": [
    "def clean_sentence(sent: str):\n",
    "    '''Function that takes in a string and outputs a cleaned version without non-alphabetic\n",
    "    characters'''\n",
    "    \n",
    "    # replace non-alphabetic characters with spaces\n",
    "    sent = re.sub('[^A-Za-z]', ' ', sent)\n",
    "    sent = re.sub('\\s+', ' ', sent)\n",
    "    \n",
    "    return sent.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbb6f8",
   "metadata": {
    "id": "c0fbb6f8"
   },
   "source": [
    "First let's look at a quick example to check that this function works as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77aec6d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77aec6d0",
    "outputId": "228c9cde-03e1-44bd-d15f-b064b52e82e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "METHODS The effects of TMZ and BG on apoptosis , cell growth , the mitotic index , cell cycle distribution , and protein expression were studied by TUNEL , cell counting , flow cytometry , and Western blot analysis , respectively .\n",
      "\n",
      "Cleaned text:\n",
      "methods the effects of tmz and bg on apoptosis cell growth the mitotic index cell cycle distribution and protein expression were studied by tunel cell counting flow cytometry and western blot analysis respectively\n"
     ]
    }
   ],
   "source": [
    "# pull out a random sentence from the dataset\n",
    "sample_sentence = train_rawtext.sentence.sample(1).tolist().pop()\n",
    "print(f'Raw text:\\n{sample_sentence}')\n",
    "cleaned_sentence = clean_sentence(sample_sentence)\n",
    "print(f'\\nCleaned text:\\n{cleaned_sentence}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafdc7f",
   "metadata": {
    "id": "0aafdc7f"
   },
   "source": [
    "We then apply the text cleaning function in a vectorised manner to all the sentences in our dataset. This is made easy by the ``apply`` method in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893c5118",
   "metadata": {
    "id": "893c5118"
   },
   "outputs": [],
   "source": [
    "train_cleantext = train_rawtext.assign(sentence=train_rawtext.sentence.apply(clean_sentence))\n",
    "dev_cleantext = dev_rawtext.assign(sentence=dev_rawtext.sentence.apply(clean_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bc7f3",
   "metadata": {
    "id": "bf7bc7f3"
   },
   "source": [
    "We also need to numerically encode the class labels, as the classifier expects integer labels, i.e. $\\{0,1\\}$ for binary classification, $\\{0,10\\}$ in our case. All we need to do is define an ordering of the labels and Python will assign integers to them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86515189",
   "metadata": {
    "id": "86515189"
   },
   "outputs": [],
   "source": [
    "abbrev_labels = ('NONE', 'PS', 'GS', 'CD', 'RI', 'A', 'IM', 'GI', 'TPI', 'CE', 'ID')\n",
    "train_cleantext.labels = train_cleantext.labels.apply(lambda x: abbrev_labels.index(x))\n",
    "dev_cleantext.labels = dev_cleantext.labels.apply(lambda x: abbrev_labels.index(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712eadae",
   "metadata": {
    "id": "712eadae"
   },
   "source": [
    "We now have a new version of the datasets which are ready for processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "246c9a93",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "246c9a93",
    "outputId": "7e54fd8c-71cf-4e40-dc68-82f5450c72f2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11724768_s0</td>\n",
       "      <td>ghrelin was identified in the stomach as an en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11724768_s1</td>\n",
       "      <td>ghs r is found in various tissues but its func...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11724768_s2</td>\n",
       "      <td>here we show that ghs r is found in hepatoma c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11724768_s3</td>\n",
       "      <td>exposure of these cells to ghrelin caused up r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11724768_s4</td>\n",
       "      <td>unlike insulin ghrelin inhibited akt kinase ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                           sentence  labels\n",
       "0  11724768_s0  ghrelin was identified in the stomach as an en...       0\n",
       "1  11724768_s1  ghs r is found in various tissues but its func...       0\n",
       "2  11724768_s2  here we show that ghs r is found in hepatoma c...       0\n",
       "3  11724768_s3  exposure of these cells to ghrelin caused up r...       1\n",
       "4  11724768_s4  unlike insulin ghrelin inhibited akt kinase ac...       0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cleantext.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725c9b23",
   "metadata": {
    "id": "725c9b23"
   },
   "source": [
    "## Tokenisation\n",
    "\n",
    "The `tokenizers` library provides tools to tokenise input text data according to the model that will be used to process them (see [the documentation](https://huggingface.co/docs/transformers/tokenizer_summary) for more details).\n",
    "\n",
    "In this assignment we will used a pre-trained version of the transformer language model BERT that is adapted for biomedical tasks, known as BlueBERT, which will be described in more detail in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "361e5c57",
   "metadata": {
    "id": "361e5c57"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658df45c",
   "metadata": {
    "id": "658df45c"
   },
   "source": [
    "**Exercise:** What does ``L-12_H-768_A-12`` signify in the model name?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc5b90",
   "metadata": {
    "id": "b3cc5b90"
   },
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680749ab",
   "metadata": {},
   "source": [
    "The string \"L-12_H-768_A-12\" is a common naming convention used to describe the architecture of the BERT (Bidirectional Encoder Representations from Transformers) model. BERT is a transformer-based language model developed by Google that has been widely used for natural language processing (NLP) tasks.\n",
    "\n",
    "The string \"L-12_H-768_A-12\" indicates that the BERT model has 12 layers (L-12), each with a hidden size of 768 (H-768), and 12 attention heads (A-12). The hidden size refers to the number of neurons or units in each layer of the BERT model, and the attention heads refer to the number of attention mechanisms used by the model.\n",
    "\n",
    "Here is a breakdown of the different components of the string:\n",
    "\n",
    "- L: The number of layers in the BERT model. A larger number of layers typically results in a more powerful model, but also requires more computational resources to train and run.\n",
    "\n",
    "- H: The hidden size or number of neurons in each layer of the BERT model. A larger hidden size typically results in a more powerful model, but also requires more computational resources to train and run.\n",
    "\n",
    "- A: The number of attention heads used by the BERT model. Attention mechanisms are used to weight the importance of different parts of the input text when generating an output. A larger number of attention heads typically results in a more powerful model, but also requires more computational resources to train and run.\n",
    "\n",
    "The string \"L-12_H-768_A-12\" is just one example of a BERT model architecture, and there are many other variations of BERT models with different numbers of layers, hidden sizes, and attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d994cee6",
   "metadata": {
    "id": "d994cee6"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc45d84",
   "metadata": {
    "id": "edc45d84"
   },
   "source": [
    "Because different input sequences have different lengths, their encodings cannot all be used in the same [tensor](https://pytorch.org/docs/stable/tensors.html) object as they are. That's why we need to fix a parameter (``MAX_LEN``) to specify the sequence length we would like to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75bd338",
   "metadata": {
    "id": "c75bd338"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6820443c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "1de341d508104320ae5ccce838d29e42",
      "cf73f65b7a644cb8ab79d68f47acd53a",
      "f479dadb2903441ca38ab9126a32f671",
      "91cc47863c2b493ea9347d446deeace7",
      "080b18dd51734b94a18952173ee097c1",
      "ad53e3b099154e78bcefe52ae23c6259",
      "3613c956e1bc46ff82a53838632b8bb3",
      "b7b4ff91ac644290afe81b21c990fa0b",
      "65caa76ec44444bfbbb10a61ec318703",
      "5f9595690be54a3cb3d1236cf999988e",
      "622147768f164c5f81d7f2ae8d98dad5",
      "dff856c4819143909da7d63d5b021774",
      "f8805fa6d37b44e6a825af5344dffc5f",
      "08e041cd768443bd9bc8fb8fbe6e3eb6",
      "eb08f2ea685e4702bbbd7b3fd0bf23d2",
      "33430f7b774b428ca7b0b91f7d9ec533",
      "c01db38c8084477ea9eb0281e0ddfcbc",
      "62d0611ad9f7430e8f8a8797169fc29e",
      "e7612d3bc2424d5eb6294ec163b0fb7e",
      "2654b8d94bc241438b7bea55e9bf285a",
      "290e613279c04556941f0e0ecda9cc0c",
      "c26bf4f0ec2d4f9ab187332a2b385ceb"
     ]
    },
    "id": "6820443c",
    "outputId": "a7783f5b-b1a2-454e-e7b4-0a2f15756655"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9aab48",
   "metadata": {
    "id": "bd9aab48"
   },
   "source": [
    "The output of the tokenizer (an object of type ``BatchEncoding``) has two main components we should focus on;\n",
    "- ``input_ids``: These are token indices, numerical representations of tokens based on the model's vocabulary.\n",
    "- ``attention_mask``: A binary tensor indicating to the model which tokens it should attend to, for sequences shorter than ``MAX_LEN`` that need to be padded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91d36eaa",
   "metadata": {
    "id": "91d36eaa"
   },
   "outputs": [],
   "source": [
    "def do_tokenisation(list_, max_length):\n",
    "    tokeniser_output = tokenizer(\n",
    "        list_,\n",
    "        padding='max_length',  # pad shorter sequences to max_length\n",
    "        truncation=True,  # truncate longer sequences to max_length\n",
    "        max_length=max_length\n",
    "    )\n",
    "    for encoding in tokeniser_output:\n",
    "        tokeniser_output[encoding] = torch.tensor(tokeniser_output[encoding])\n",
    "        \n",
    "    return tokeniser_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2107cd90",
   "metadata": {
    "id": "2107cd90"
   },
   "outputs": [],
   "source": [
    "train_tokeniser_output = do_tokenisation(train_cleantext.sentence.tolist(), MAX_LEN)\n",
    "dev_tokeniser_output = do_tokenisation(dev_cleantext.sentence.tolist(), MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7560e2",
   "metadata": {
    "id": "de7560e2"
   },
   "source": [
    "Let's have a look at an example of the encodings produced by the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9569267",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9569267",
    "outputId": "f0994a9f-3586-4e8a-f1cb-e18c17583854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence:\n",
      "ghrelin was identified in the stomach as an endogenous ligand specific for the growth hormone secretagogue receptor ghs r\n",
      "\n",
      "Tokens:\n",
      "['[CLS]', 'g', '##hre', '##lin', 'was', 'identified', 'in', 'the', 'stomach', 'as', 'an', 'end', '##ogen', '##ous', 'ligand', 'specific', 'for', 'the', 'growth', 'hormone', 'secret', '##ago', '##gue', 'receptor', 'g', '##hs', 'r', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n",
      "Input IDs:\n",
      "tensor([  101,  1043, 28362,  4115,  2001,  4453,  1999,  1996,  4308,  2004,\n",
      "         2019,  2203, 23924,  3560, 27854,  3563,  2005,  1996,  3930, 18714,\n",
      "         3595, 23692,  9077, 10769,  1043,  7898,  1054,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "Attention Mask:\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_idx = 0\n",
    "original_sentence = train_cleantext.sentence[batch_idx]\n",
    "print(f'Input sentence:\\n{original_sentence}\\n')\n",
    "tokens = train_tokeniser_output.tokens(batch_idx)\n",
    "print(f'Tokens:\\n{tokens}\\n')\n",
    "input_ids = train_tokeniser_output['input_ids'][batch_idx]\n",
    "print(f'Input IDs:\\n{input_ids}\\n')\n",
    "attention_mask = train_tokeniser_output['attention_mask'][batch_idx]\n",
    "print(f'Attention Mask:\\n{attention_mask}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cl3JdGtVG75y",
   "metadata": {
    "id": "cl3JdGtVG75y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['[CLS]', 'g', '##hre', '##lin', 'was', 'identified', 'in', 'the', 'stomach', 'as', 'an', 'end', '##ogen', '##ous', 'ligand', 'specific', 'for', 'the', 'growth', 'hormone', 'secret', '##ago', '##gue', 'receptor', 'g', '##hs', 'r', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = train_tokeniser_output.tokens(0)\n",
    "print(f'Tokens:\\n{tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729be2d6",
   "metadata": {
    "id": "729be2d6"
   },
   "source": [
    "#### Exercises\n",
    "\n",
    "1. What does the output of the tokeniser represent? \n",
    "2. What are the special tokens introduced by the tokenizer? What is their function?\n",
    "3. Why do some tokens start with ## (for example, ##rea ##der)?\n",
    "4. Note that the tokenizer that we use has been pre-trained. Why does a tokenizer like this require pre-training?\n",
    "5. The architecture we will be using to process the text handles sequences with a maximum length of 512 (`MAX_LEN=512`). Check if this length is suitable for our data. In other words, look at the length distribution of the texts we have to classify and decide whether it would be advantageous to reduce `MAX_LEN` in order to reduce the processing time of the sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a053308",
   "metadata": {
    "id": "9a053308"
   },
   "source": [
    "**Answers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa845fee",
   "metadata": {},
   "source": [
    "**1-What does the output of the tokeniser represent?**\n",
    "The AutoTokenizer is a class in the transformers library that is used to automatically preprocess text data and convert it into a numerical representation that can be fed into a transformer model.\n",
    "\n",
    "The AutoTokenizer takes a piece of text as input and output a list of tokens, where each token represents a word or subword in the input text. The tokens are produced by applying a process called tokenization, which involves splitting the input text into a sequence of tokens. The AutoTokenizer class uses a pre-trained tokenization model to perform this task, and it can handle a wide range of natural language text.\n",
    "\n",
    "The output of the AutoTokenizer can be used as input to a transformer model for a variety of NLP tasks, such as language translation, text classification, and question answering. The transformer model processes the tokens and produces a prediction or output based on the input text.\n",
    "\n",
    "The output of the tokenizer (an object of type BatchEncoding) contains tokens, input_id an attention_mask.... so it is a sequence of tokens that have been converted into numerical form using a process called tokenization. The numerical representation of the tokens is called the input ID. The input ID is a list of integers, where each integer corresponds to a token in the input text. The integers are assigned to each token according to a pre-defined vocabulary, which maps each token to a unique integer.\n",
    "\n",
    "The attention mask is a binary sequence that is used to specify which tokens in the input text should be attended to by the transformer model. The attention mask is used to exclude padding tokens (tokens that are added to the input text to ensure that all input sequences have the same length) from the model's attention.\n",
    "\n",
    "\n",
    "**2-What are the special tokens introduced by the tokenizer? What is their function?**\n",
    "\n",
    "The BERT tokenizer introduces several \"special tokens\" that have specific functions in the BERT model. These special tokens are added to the input text as part of the tokenization process and are used to provide additional information to the model about the input text.\n",
    "\n",
    "Here are some of the special tokens that are used by the BERT tokenizer:\n",
    "\n",
    "[CLS]: This token is added to the beginning of the input text and is used to represent the entire input sequence. It is used by the BERT model to generate a single output for the entire input sequence.\n",
    "\n",
    "[SEP]: This token is added between the input text and any additional context (such as a question or answer in a Q&A task). It is used to separate the input text from the additional context and to indicate the end of the input text.\n",
    "\n",
    "[PAD]: This token is added to the input text to ensure that all input sequences have the same length. It is used to pad the input text to the desired length.\n",
    "\n",
    "[UNK]: This token is used to represent out-of-vocabulary (OOV) words that are not included in the BERT vocabulary.\n",
    "\n",
    "The BERT tokenizer also includes a technique called wordpiece tokenization, which involves breaking words into subwords if they are not included in the BERT vocabulary. This allows the BERT model to handle a wider range of words and to capture the meaning of rare or OOV words by combining subwords that represent their meanings.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**3-Why do some tokens start with ## (for example, ##rea ##der)?**\n",
    "\n",
    "In natural language processing (NLP), it is common to use a special notation to indicate that a token is a subword or a part of a word that has been split by the tokenizer. This is often done to handle words that are not in the tokenizer's vocabulary, or to split compound words into smaller units that are more useful for certain NLP tasks.\n",
    "\n",
    "One way to indicate that a token is a subword is to prefix it with a special symbol, such as \"##\". For example, if you have a tokenizer that is trained on a vocabulary of common English words, and you want to tokenize the following compound word:\n",
    "\n",
    "\"modulates\"\n",
    "\n",
    "The tokenizer might split it into the following tokens:\n",
    "\n",
    "\"mod\", \"##ulates\"\n",
    "\n",
    "The \"##\" prefix indicates that the two tokens together represent the original compound word \"bicycle\".\n",
    "\n",
    "This approach is often used in combination with subword-based tokenizers, such as the BPE (byte pair encoding) or WordPiece tokenizers, which split words into subwords based on statistical patterns in the training data. By breaking words into smaller units, these tokenizers can handle out-of-vocabulary words and rare words more effectively, and can also reduce the size of the vocabulary needed for a given NLP task.\n",
    "\n",
    "However, keep in mind that the specific notation and rules for handling subwords can vary depending on the specific tokenizer and NLP toolkit being used. You should refer to the documentation of the specific tool you are using for more information.\n",
    "\n",
    "\n",
    "\n",
    "**4-Note that the tokenizer that we use has been pre-trained. Why does a tokenizer like this require pre-training?**\n",
    "\n",
    "Pre-training a tokenizer involves training the tokenizer on a large dataset of text to learn how to tokenize the text in a way that is effective for a specific NLP task. Pre-trained tokenizers have been trained on large datasets of text and have learned how to perform tokenization in a way that is effective for a specific NLP task.\n",
    "\n",
    "There are several reasons why a tokenizer in NLP requires pre-training:\n",
    "\n",
    "1-Tokenization is a complex task that requires a deep understanding of natural language. Pre-trained tokenizers have been trained on large datasets of text (which is very resource demanding and costly) and have learned how to perform tokenization in a way that is effective for a specific NLP task.\n",
    "\n",
    "2-Pre-trained tokenizers have been trained on large datasets of text and have learned how to handle a wide range of natural language text. This makes them more effective at tokenizing text than tokenizers that have not been pre-trained.\n",
    "\n",
    "3-Pre-trained tokenizers can handle out-of-vocabulary (OOV) words and rare words more effectively than tokenizers that have not been pre-trained. This is because pre-trained tokenizers have learned how to split words into subwords or characters to represent their meanings.\n",
    "\n",
    "4-Pre-trained tokenizers can improve the performance of NLP models by providing a high-quality representation of the input text. This is because pre-trained tokenizers have been trained on large datasets of text and have learned how to tokenize the text in a way that is effective for a specific NLP task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**5-The architecture we will be using to process the text handles sequences with a maximum length of 512 (MAX_LEN=512). Check if this length is suitable for our data. In other words, look at the length distribution of the texts we have to classify and decide whether it would be advantageous to reduce MAX_LEN in order to reduce the processing time of the sequences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b86c7869",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b86c7869",
    "outputId": "108be3b4-3821-497d-a6d5-bf486b3f6f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "## YOUR CODE HERE (Exercise 5)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "L1=train_tokeniser_output.attention_mask.tolist()\n",
    "print(max(np.sum(L1,1)))\n",
    "L2=dev_tokeniser_output.attention_mask.tolist()\n",
    "print(max(np.sum(L2,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35cecd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 72., 275., 461., 330., 180.,  97.,  46.,  20.,   8.,   7.]),\n",
       " array([ 6. , 14.4, 22.8, 31.2, 39.6, 48. , 56.4, 64.8, 73.2, 81.6, 90. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.sum(L2,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7621722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 187., 1571., 3124., 2362., 1738.,  842.,  374.,  182.,   72.,\n",
       "          75.]),\n",
       " array([ 3. , 11.7, 20.4, 29.1, 37.8, 46.5, 55.2, 63.9, 72.6, 81.3, 90. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.sum(L1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ce364",
   "metadata": {},
   "source": [
    "We can see that the largest sentence in our training and validation dataset is 149 so setting the MAX_LEN=150 seems plausible and would avoid using uselessly more computing power than we need. we can  possibly lower this value even more if it does not impact negatively the performance of our model and this would require further testing and comparing. Also according to the histograms it would be advisable to use Max_LEN between 80 and 100 if it dows not effect significatly our results ad we would like to have a better computing performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b64975",
   "metadata": {
    "id": "a3b64975"
   },
   "source": [
    "## Language Model\n",
    "\n",
    "In this section we will use the Hugging Face API to load the pre-trained weights of a transformer language model for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fefe335a",
   "metadata": {
    "id": "fefe335a"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a15f46",
   "metadata": {
    "id": "e3a15f46"
   },
   "source": [
    "The ``config`` attribute contains hyperparameter values including vocabulary size, dropout probabilities, and architectural specifications; feel free to inspect them and try to understand what they mean and how they relate to the course material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3835a877",
   "metadata": {
    "id": "3835a877"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"_name_or_path\": \"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.25.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549a38b",
   "metadata": {
    "id": "5549a38b"
   },
   "source": [
    "By default, the output size of the classification layer (corresponding to the number of different classes we want to identify) is 2. However, we have 10 hallmarks of cancer to classify, plus `NONE`, so we have an 11-class classification problem. This means we need to tell the model to expect 11 different class labels and adjust its output size accordingly. We do this by setting the `num_labels` configuration parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ca6592",
   "metadata": {
    "id": "41ca6592"
   },
   "outputs": [],
   "source": [
    "model_config.num_labels = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dceea",
   "metadata": {
    "id": "7f6dceea"
   },
   "source": [
    "The ``model`` object we use here is an instantiation of the pre-trained [BlueBERT](https://github.com/ncbi-nlp/bluebert) model with an extra classification layer added. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c61896f",
   "metadata": {
    "id": "5c61896f"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_config(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35228fa0",
   "metadata": {
    "id": "35228fa0"
   },
   "source": [
    "### Optimiser\n",
    "Neural networks require an optimisation strategy to implement backpropagation. We will use the [Adam](https://arxiv.org/pdf/1412.6980.pdf) algorithm, a widely-used method for transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c461814d",
   "metadata": {
    "id": "c461814d"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6397d85f",
   "metadata": {
    "id": "6397d85f"
   },
   "source": [
    "At this point in the implementation, we choose the learning rate to use in backpropagation: a parameter that controls the step sizes taken across the loss landscape by the optimiser at each step of training. This is a hyperparameter that has no _a priori_ optimal value; you can play around with the value to see how it affects the performance of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3dcab6cf",
   "metadata": {
    "id": "3dcab6cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WORK\\AI4oneH\\Anaconda\\envs\\tf-gpu\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# the constructor function requires the parameters of the language model and the learning rate as input\n",
    "optimiser = AdamW(tuple(model.parameters()), lr=2e-5,eps=2e-30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e25778",
   "metadata": {
    "id": "43e25778"
   },
   "source": [
    "### Custom Datasets & Data Loaders\n",
    "For efficiency, we will use the Pytorch ``DataLoader`` object to pass the tokens to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36ad96a9",
   "metadata": {
    "id": "36ad96a9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a60aeb",
   "metadata": {
    "id": "17a60aeb"
   },
   "source": [
    "For most Pytorch projects, you will need to define a custom dataset class that subclasses the built-in one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d934035",
   "metadata": {
    "id": "0d934035"
   },
   "outputs": [],
   "source": [
    "# here we define a simple subclass of the Pytorch Dataset object: the DataLoader by default will access the amount\n",
    "# of data points stored by instances of this subclass, as well as indexing it, so we have to overwrite the __len__\n",
    "# and __getitem__ methods to make sure it will behave as we would like\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, tokeniser_output: transformers.BatchEncoding, labels: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.__dict__.update(tokeniser_output)\n",
    "        self.labels = labels\n",
    "        self.keys = tuple(tokeniser_output.keys()) + ('labels',)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return {key: getattr(self, key)[idx] for key in self.keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d27d9976",
   "metadata": {
    "id": "d27d9976"
   },
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_cleantext.labels.values, dtype=torch.int64)\n",
    "dev_labels = torch.tensor(dev_cleantext.labels.values, dtype=torch.int64)\n",
    "\n",
    "train_dataset = ClassificationDataset(train_tokeniser_output, train_labels)\n",
    "dev_dataset = ClassificationDataset(dev_tokeniser_output, dev_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9c11da",
   "metadata": {
    "id": "0e9c11da"
   },
   "source": [
    "At this point we select another hyperparameter, the _batch size_, i.e. the number of sentences to process at each training step. A higher batch size will result in more effective backpropagation steps, as the model will have \"seen\" more training examples before calculating the adjustment to make to its parameters, but a lower batch size will result in more efficient training as not as many examples need to be loaded into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "411b5f8d",
   "metadata": {
    "id": "411b5f8d"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306c33f",
   "metadata": {
    "id": "0306c33f"
   },
   "source": [
    "## Training a Classifier\n",
    "To fine-tune the BlueBERT word embeddings for our document classification task, we use the ``[CLS]`` token from the final layer of the transformer neural network to predict the relevant class for each sentence. To output class probabilities based on the word embedding vectors, a linear prediction layer is stacked on top of the transformer network, which will learn weight parameters $w$ that correspond to the optimal transformation of the ``[CLS]`` vector into a vector of scores for each class in the output space (usually called _logits_). To generate class probabilities from the logit vector, the [softmax function](https://en.wikipedia.org/wiki/Softmax_function) is applied:\n",
    "\n",
    "$$\n",
    "\\phi_w:\\mathbb{R}^{d_{\\text{EMB}}}\\rightarrow\\mathbb{R}^{d_{\\text{CLASS}}} \\\\\n",
    "\\text{Classifier}\\left(x_{\\text{CLS}}\\right)=\\text{softmax}\\left(\\phi_w\\left(x_{\\text{CLS}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "For the model we use in this tutorial, we have embedding dimension $d_{\\text{EMB}}=768$ and because we have ten hallmarks of cancer plus the absence of a hallmark, we have output dimension $d_{\\text{CLASS}}=11$.\n",
    "\n",
    "To predict a single class, however, we just need to pick out the dimension of the logit vector with the highest score:\n",
    "\n",
    "$$\\text{Predicted Class}=\\text{argmax}\\left(\\phi\\left(x_{\\text{CLS}}\\right)\\right)$$\n",
    "\n",
    "### Using GPUs\n",
    "Graphical Processing Units (GPUs) are optimised for very fast parallel matrix calculations and are thus widely used for training neural networks. In practice, for models of the scale of BERT, GPUs are necessary to be able to perform experiments in reasonable timeframes (although pre-training still often requires several days of runtime on multiple GPUs).\n",
    "\n",
    "Communicating data & instructions for calculations between CPUs and GPUs can be a tricky engineering task, especially when there are multiple machines involved, but luckily, once again, Hugging Face provides a very simple API that allows us to easily implement GPU training and parallelism, called [Accelerate](https://huggingface.co/docs/accelerate/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dcb6cd0",
   "metadata": {
    "id": "4dcb6cd0"
   },
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ef436",
   "metadata": {
    "id": "959ef436"
   },
   "source": [
    "With Accelerate, we simply wrap our training objects using the ``prepare`` method - this creates GPU-compatible objects that can be used for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e42794ff",
   "metadata": {
    "id": "e42794ff"
   },
   "outputs": [],
   "source": [
    "if device is not None:\n",
    "    model, optimiser, train_dataloader, dev_dataloader = accelerator.prepare(\n",
    "        model, optimiser, train_dataloader, dev_dataloader\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3de0ca",
   "metadata": {
    "id": "eb3de0ca"
   },
   "source": [
    "### Evaluation Metrics\n",
    "The last thing to prepare before launching the training is our method of evaluating the model. The best metric to use can vary depending on the classification task in question. The standard metric for the Hallmarks of Cancer classification task is the [F1-score](https://en.wikipedia.org/wiki/F-score), which is essentially the harmonic mean of the model's precision $p$ (proportion of positive predictions that were correct) and recall $r$ (proportion of positive examples that were correctly predicted):\n",
    "\n",
    "$$F_1=2\\frac{p\\cdot r}{p+r}$$\n",
    "\n",
    "Because our classification task is quite imbalanced, we will calculate the F1-score separately for each class, then calculate the overall score of the model via a weighted average of these F1-scores (weights based on their prevalence in the training data). This is a standard way of adjusting the F1-score to compensate for class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e1feefa",
   "metadata": {
    "id": "7e1feefa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46266143",
   "metadata": {
    "id": "46266143"
   },
   "source": [
    "Now, we are (finally) ready to start training our classifier. The first thing to do is to decide how many epochs (full passes over the dataset) we would like to use. For fine-tuning BERT models, we usually consider that the model pre-training has already instilled it with a lot of the knowledge necessary to carry out the task, so we train for relatively few (2-5) epochs, but this heuristic does not necessarily apply to all problems and for many domain-specific tasks (such as this one) it may be beneficial to increase this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83800050",
   "metadata": {
    "id": "83800050"
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25eb8c1",
   "metadata": {
    "id": "c25eb8c1"
   },
   "source": [
    "The ``DataLoader`` object automatically gives us an iterator over the batches of data - basically, it has chopped the dataset up into bite-sized chunks of size ``BATCH_SIZE`` to make it more digestible for the machine. At each epoch, we iterate over all the batches. To get the evaluation metric (F1-score for the model as a whole we take the average of the score for each batch.\n",
    "\n",
    "**Exercise:** At the moment, the code below records & displays metrics only for the training set. However, we would like to see the F1-score on the development set too - this gives us a better idea of how our model is improving as the development set contains examples that the model hasn't \"seen\", i.e. hasn't used to update it's parameters.\n",
    "Adjust the code so that the development metrics are also tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c109a304",
   "metadata": {
    "id": "c109a304"
   },
   "outputs": [],
   "source": [
    "# progress bar for batches\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47e7865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_dataloader) * N_EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimiser, \n",
    "                                            num_warmup_steps = 0, \n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1172c8c",
   "metadata": {
    "id": "f1172c8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:12<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train set F1-score=0.5805648826087837\n",
      "Epoch 0: eval set F1-score=0.4298245614035088\n",
      "Epoch 1, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:18<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train set F1-score=0.5886216298384637\n",
      "Epoch 1: eval set F1-score=0.6254734848484849\n",
      "Epoch 2, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:23<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:06<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: train set F1-score=0.6188719234433828\n",
      "Epoch 2: eval set F1-score=0.6262626262626263\n",
      "Epoch 3, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:18<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:06<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: train set F1-score=0.6337544347513914\n",
      "Epoch 3: eval set F1-score=0.6262626262626263\n",
      "Epoch 4, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:14<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  8.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: train set F1-score=0.6425822188593633\n",
      "Epoch 4: eval set F1-score=0.6262626262626263\n",
      "Epoch 5, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:13<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  8.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: train set F1-score=0.670070419872417\n",
      "Epoch 5: eval set F1-score=0.7416666666666667\n",
      "Epoch 6, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:17<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: train set F1-score=0.7075114792247085\n",
      "Epoch 6: eval set F1-score=0.7590996168582377\n",
      "Epoch 7, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:18<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: train set F1-score=0.7400189838710384\n",
      "Epoch 7: eval set F1-score=0.7540491118077325\n",
      "Epoch 8, training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 329/329 [02:17<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 47/47 [00:05<00:00,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: train set F1-score=0.7675733661899924\n",
      "Epoch 8: eval set F1-score=0.7540491118077325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_f1_scores_train = []\n",
    "    print(f'Epoch {epoch}, training...')\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        # forward pass\n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels']\n",
    "        )\n",
    "        \n",
    "        # backpropagation\n",
    "        accelerator.backward(outputs.loss)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimiser.step()\n",
    "        scheduler.step()\n",
    "        optimiser.zero_grad()\n",
    "        \n",
    "        # metrics\n",
    "        logits = accelerator.gather(outputs.logits).cpu()  # retrieve data from the GPU\n",
    "        labels = batch['labels'].cpu()\n",
    "        predictions = logits.argmax(-1)\n",
    "        score = f1_score(labels, predictions, average='weighted')\n",
    "        epoch_f1_scores_train.append(score)\n",
    "    \n",
    "    epoch_f1_score_train = sum(epoch_f1_scores_train) / len(epoch_f1_scores_train)\n",
    "        \n",
    "    model.eval()\n",
    "    print(f'Epoch {epoch}, evaluating...')\n",
    "    for batch in tqdm(dev_dataloader):\n",
    "        with torch.no_grad():  # this context manager deactivates the backpropagation-related elements of the tensors\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels']\n",
    "            )\n",
    "        \n",
    "        ##  YOUR CODE HERE\n",
    "        # metrics\n",
    "        epoch_f1_scores_eval = []\n",
    "        logits = accelerator.gather(outputs.logits).cpu()  # retrieve data from the GPU\n",
    "        labels = batch['labels'].cpu()\n",
    "        predictions = logits.argmax(-1)\n",
    "        score = f1_score(labels, predictions, average='weighted')\n",
    "        epoch_f1_scores_eval.append(score)\n",
    "    \n",
    "    epoch_f1_score_eval = sum(epoch_f1_scores_eval) / len(epoch_f1_scores_eval)\n",
    "        \n",
    "    print(f'Epoch {epoch}: train set F1-score={epoch_f1_score_train}')    \n",
    "    print(f'Epoch {epoch}: eval set F1-score={epoch_f1_score_eval}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a99f9",
   "metadata": {},
   "source": [
    "**It is possible for the F1 score on the development (dev) set to be higher than the F1 score on the test set. The dev set is used to tune the model and evaluate its performance during training, while the test set is used to evaluate the model's generalization ability after training is complete.**\n",
    "\n",
    "**It is common for the model to perform better on the dev set because it has seen the data before and has been specifically trained to optimize its performance on that set. However, the test set contains data that the model has not seen before, so it is a better measure of the model's ability to generalize to new, unseen data.**\n",
    "\n",
    "**It is important to keep this in mind when evaluating the performance of a model and to use the test set as the final evaluation of the model's performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527db57",
   "metadata": {
    "id": "7527db57"
   },
   "source": [
    "Hopefully (!) we should see the performance metrics start to improve with each epoch.\n",
    "\n",
    "Now that we can see the model's performance on the development set, try to re-run the training with varying values of the hyperparameters;\n",
    "- number of epochs (``N_EPOCHS``)\n",
    "- learning rate (``lr`` parameter in the optimiser)\n",
    "\n",
    "What changes do you notice in model performance?\n",
    "\n",
    "**In the context of training a classification model using BERT, the learning rate determines the step size at which the optimizer makes updates to the model parameters. A higher learning rate can lead to faster convergence, but it can also cause the model to oscillate or even diverge. On the other hand, a lower learning rate can result in slower convergence, but it may lead to a more stable and precise optimization process.**\n",
    "\n",
    "**The number of epochs refers to the number of times the model is trained on the entire training dataset. Increasing the number of epochs can allow the model to continue learning and potentially improve its performance, but it can also increase the risk of overfitting, where the model becomes too specialized to the training data and performs poorly on unseen data.**\n",
    "\n",
    "**In general, it is important to find a good balance between the learning rate and the number of epochs that allows the model to converge efficiently and perform well on the evaluation dataset. This can involve some experimentation, such as trying different combinations of learning rates and epochs and comparing the results. It can also be helpful to use techniques such as learning rate scheduling, where the learning rate is adjusted during training based on the progress of the optimization process.**\n",
    "\n",
    "**It is worth noting that when using a lower value of lr we need a higher N_EPOCHS to achieve the expected results**\n",
    "\n",
    "\n",
    "**Note:** in order to compare the effects of changing the hyperparameters, training needs to be restarted from the beginning, so make sure to reinitialise all variables each time.\n",
    "\n",
    "### Predictions on Unseen Data\n",
    "Now that our model has been trained and we are happy with our choice of hyperparameters, we evaluate its performance on the _hold-out set_ or _test set_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70dfe6ad",
   "metadata": {
    "id": "70dfe6ad"
   },
   "outputs": [],
   "source": [
    "test_rawtext = read_csv('hoc-dataset-main/test.tsv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea13fc6",
   "metadata": {
    "id": "eea13fc6"
   },
   "source": [
    "**Exercise:** Evaluate the model on this new data.\n",
    "\n",
    "_Hint:_ We load and process this data in the very same way as the training and development set, and evaluate the model in the same way we did on the development set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d9a7af9",
   "metadata": {
    "id": "7d9a7af9"
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing: replace \"None\" with the correct functions\n",
    "test_cleantext = test_rawtext.assign(sentence=test_rawtext.sentence.apply(clean_sentence))\n",
    "test_tokeniser_output = do_tokenisation(test_cleantext.sentence.tolist(), MAX_LEN)\n",
    "test_cleantext.labels = test_cleantext.labels.apply(lambda x: abbrev_labels.index(x))\n",
    "test_labels = torch.tensor(test_cleantext.labels.values, dtype=torch.int64)\n",
    "test_dataset = ClassificationDataset(test_tokeniser_output, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d57fa035",
   "metadata": {
    "id": "d57fa035"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 91/91 [00:11<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set F1-score=0.7544642857142857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation (don't forget to place the dataloader on the GPU!)\n",
    "# YOUR CODE HERE\n",
    "if device is not None:\n",
    "    model, optimiser, test_dataloader = accelerator.prepare(\n",
    "        model, optimiser, test_dataloader\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    print('evaluating...')\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        with torch.no_grad():  # this context manager deactivates the backpropagation-related elements of the tensors\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels']\n",
    "            )\n",
    "        \n",
    "       \n",
    "        # metrics\n",
    "        epoch_f1_scores_test = []\n",
    "        logits = accelerator.gather(outputs.logits).cpu()  # retrieve data from the GPU\n",
    "        labels = batch['labels'].cpu()\n",
    "        predictions = logits.argmax(-1)\n",
    "        score = f1_score(labels, predictions, average='weighted')\n",
    "        epoch_f1_scores_test.append(score)\n",
    "    \n",
    "    epoch_f1_score_test = sum(epoch_f1_scores_test) / len(epoch_f1_scores_test)\n",
    "    print(f'test set F1-score={epoch_f1_score_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2efd53f",
   "metadata": {
    "id": "f2efd53f"
   },
   "source": [
    "## Further Work: Improving Classification\n",
    "Now that we have built a classifier in a basic way, we can look at ways to improve the model to make it more robust and improve its real-world performance on unseen examples. There is no fixed strategy that is guaranteed to work here: deep learning is a highly empirical discipline, and usually you have to try things out to see whether or not they will work, without much in the way of theoretical performance guarantees.\n",
    "\n",
    "Here are some strategies you could investigate and potentially adapt to this task to see what effect they have on performance;\n",
    "- [Cross-validation](https://learn.g2.com/cross-validation)\n",
    "- [Learning Rate Scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "- Inspect the performance on each hallmark individually - are there any particular patterns the model has trouble classifying correctly?\n",
    "- Try evaluating the model using some other metrics: the F1-score combines precision and recall, so it may be interesting to drill down and see if there is an imbalance between the two. Another widely used metric for classification problems (particularly imbalanced ones) is the area under the [Receiver Operating Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve (AUROC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c16bff7",
   "metadata": {
    "id": "4c16bff7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d531dcbf",
   "metadata": {
    "id": "d531dcbf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "EnvQ5AfZgerF",
   "metadata": {
    "id": "EnvQ5AfZgerF"
   },
   "source": [
    "#Submitting your work\n",
    "\n",
    "To submit the completed Colab notebook, simply click on the \"Share\" button in the top-right corner of the editor. Ensure that the \"General Access\" setting is set to \"Anyone with the link\" and send the link via email to the address below with the subject line \"[Lab2 AI4H]Name\", replacing \"Name\" with your surname.\n",
    "\n",
    "> aidan.mannion@univ-grenoble-alpes.fr\n",
    "\n",
    "It would also be helpful if you changed the name of the notebook (simply click on the file name in the top left corner) to include your name :) thank you!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "729be2d6",
    "35228fa0",
    "43e25778",
    "eb3de0ca",
    "7527db57"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "080b18dd51734b94a18952173ee097c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08e041cd768443bd9bc8fb8fbe6e3eb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7612d3bc2424d5eb6294ec163b0fb7e",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2654b8d94bc241438b7bea55e9bf285a",
      "value": 231508
     }
    },
    "1de341d508104320ae5ccce838d29e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cf73f65b7a644cb8ab79d68f47acd53a",
       "IPY_MODEL_f479dadb2903441ca38ab9126a32f671",
       "IPY_MODEL_91cc47863c2b493ea9347d446deeace7"
      ],
      "layout": "IPY_MODEL_080b18dd51734b94a18952173ee097c1"
     }
    },
    "2654b8d94bc241438b7bea55e9bf285a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "290e613279c04556941f0e0ecda9cc0c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33430f7b774b428ca7b0b91f7d9ec533": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3613c956e1bc46ff82a53838632b8bb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f9595690be54a3cb3d1236cf999988e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "622147768f164c5f81d7f2ae8d98dad5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "62d0611ad9f7430e8f8a8797169fc29e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "65caa76ec44444bfbbb10a61ec318703": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91cc47863c2b493ea9347d446deeace7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5f9595690be54a3cb3d1236cf999988e",
      "placeholder": "â",
      "style": "IPY_MODEL_622147768f164c5f81d7f2ae8d98dad5",
      "value": " 313/313 [00:00&lt;00:00, 8.56kB/s]"
     }
    },
    "ad53e3b099154e78bcefe52ae23c6259": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7b4ff91ac644290afe81b21c990fa0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01db38c8084477ea9eb0281e0ddfcbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c26bf4f0ec2d4f9ab187332a2b385ceb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cf73f65b7a644cb8ab79d68f47acd53a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad53e3b099154e78bcefe52ae23c6259",
      "placeholder": "â",
      "style": "IPY_MODEL_3613c956e1bc46ff82a53838632b8bb3",
      "value": "Downloading: 100%"
     }
    },
    "dff856c4819143909da7d63d5b021774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8805fa6d37b44e6a825af5344dffc5f",
       "IPY_MODEL_08e041cd768443bd9bc8fb8fbe6e3eb6",
       "IPY_MODEL_eb08f2ea685e4702bbbd7b3fd0bf23d2"
      ],
      "layout": "IPY_MODEL_33430f7b774b428ca7b0b91f7d9ec533"
     }
    },
    "e7612d3bc2424d5eb6294ec163b0fb7e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eb08f2ea685e4702bbbd7b3fd0bf23d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_290e613279c04556941f0e0ecda9cc0c",
      "placeholder": "â",
      "style": "IPY_MODEL_c26bf4f0ec2d4f9ab187332a2b385ceb",
      "value": " 232k/232k [00:00&lt;00:00, 625kB/s]"
     }
    },
    "f479dadb2903441ca38ab9126a32f671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7b4ff91ac644290afe81b21c990fa0b",
      "max": 313,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_65caa76ec44444bfbbb10a61ec318703",
      "value": 313
     }
    },
    "f8805fa6d37b44e6a825af5344dffc5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c01db38c8084477ea9eb0281e0ddfcbc",
      "placeholder": "â",
      "style": "IPY_MODEL_62d0611ad9f7430e8f8a8797169fc29e",
      "value": "Downloading: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
